{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavel-Zinkevich/english_folklore/blob/main/Parsing_wiki_preparing_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J54DOYtJnsMO"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai mwclient mwparserfromhell tiktoken"
      ],
      "metadata": {
        "id": "kkuNx4WXoDQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import mwclient  # библиотека для работы с MediaWiki API для загрузки примеров статей Википедии\n",
        "import mwparserfromhell  # Парсер для MediaWiki\n",
        "import openai  # будем использовать для токинизации\n",
        "import pandas as pd  # В DataFrame будем хранить базу знаний и результат токинизации базы знаний\n",
        "import re  # для вырезания ссылок <ref> из статей Википедии\n",
        "import tiktoken  # для подсчета токенов"
      ],
      "metadata": {
        "id": "8TGE15HFoFBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"English folklore\"\n",
        "\n",
        "# Задаем категорию и англоязычную версию Википедии для поиска\n",
        "CATEGORY_TITLE = f\"Category:{name}\"\n",
        "WIKI_SITE = \"en.wikipedia.org\""
      ],
      "metadata": {
        "id": "JHYeADiNoGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Соберем заголовки всех статей\n",
        "def titles_from_category(category: mwclient.page.Page, max_depth: int) -> set[str]:\n",
        "    \"\"\"Возвращает множество заголовков статей категории и её подкатегорий, без самих подкатегорий.\"\"\"\n",
        "    titles = set()\n",
        "    for cm in category.members():\n",
        "        time.sleep(0.1)\n",
        "        if isinstance(cm, mwclient.page.Page):\n",
        "            if cm.name.startswith(\"Category:\") and max_depth > 0:\n",
        "                # рекурсивно обрабатываем подкатегорию\n",
        "                titles.update(titles_from_category(site.pages[cm.name], max_depth - 1))\n",
        "            elif not cm.name.startswith(\"Category:\"):\n",
        "                # добавляем только обычные статьи\n",
        "                titles.add(cm.name)\n",
        "    return titles\n",
        "\n",
        "\n",
        "\n",
        "# Инициализация объекта MediaWiki\n",
        "# WIKI_SITE ссылается на англоязычную часть Википедии\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "\n",
        "# Загрузка раздела заданной категории\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "# Получение множества всех заголовков категории с вложенностью на один уровень\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "\n",
        "\n",
        "print(f\"Создано {len(titles)} заголовков статей в категории {CATEGORY_TITLE}.\")"
      ],
      "metadata": {
        "id": "P9FZQHtkoIMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем секции, которые будут отброшены при парсинге статей\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "MYgpeQaBoJzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from requests.exceptions import HTTPError\n",
        "import mwclient\n",
        "\n",
        "def all_subsections_from_title_with_retry(\n",
        "    title: str,\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
        "    site_name: str = WIKI_SITE,\n",
        "    max_retries: int = 3\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    Версия функции с повторными попытками при ошибках\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            time.sleep(1)  # Увеличьте задержку здесь\n",
        "            site = mwclient.Site(site_name)\n",
        "            page = site.pages[title]\n",
        "            text = page.text()\n",
        "\n",
        "            # Остальной код функции...\n",
        "            parsed_text = mwparserfromhell.parse(text)\n",
        "            headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "            if headings:\n",
        "                summary_text = str(parsed_text).split(headings[0])[0]\n",
        "            else:\n",
        "                summary_text = str(parsed_text)\n",
        "\n",
        "            results = [([title], summary_text)]\n",
        "            for subsection in parsed_text.get_sections(levels=[2]):\n",
        "                results.extend(\n",
        "                    all_subsections_from_section(subsection, [title], sections_to_ignore)\n",
        "                )\n",
        "            return results\n",
        "\n",
        "        except HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = 2 ** attempt  # Экспоненциальная backoff стратегия\n",
        "                print(f\"Ошибка 429, жду {wait_time} секунд перед повторной попыткой...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(f\"Не удалось получить данные для '{title}' после {max_retries} попыток\")\n",
        "\n",
        "# Используйте новую функцию в цикле\n",
        "wikipedia_sections = []\n",
        "for i, title in enumerate(titles):\n",
        "    try:\n",
        "        print(f\"Обрабатываю {i+1}/{len(titles)}: {title}\")\n",
        "        wikipedia_sections.extend(all_subsections_from_title_with_retry(title))\n",
        "        time.sleep(2)  # Увеличьте задержку между статьями\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке '{title}': {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "C6qIvJJvoLNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "with open('wikipedia_sections.pkl', 'wb') as f:\n",
        "    pickle.dump(wikipedia_sections, f)\n",
        "# with open('wikipedia_sections.pkl', 'rb') as f:\n",
        "#     wikipedia_sections_loaded = pickle.load(f)"
      ],
      "metadata": {
        "id": "sEyu5PbgoR2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_section_advanced(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    titles, text = section\n",
        "\n",
        "    # 1. Удаляем ссылки <ref>...</ref>\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # 2. Убираем шаблоны {{...}}\n",
        "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # 3. Убираем вики-ссылки [[...]] -> оставляем только текст\n",
        "    text = re.sub(r\"\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]\", r\"\\1\", text)\n",
        "\n",
        "    # 4. Убираем заголовки ==Heading==\n",
        "    text = re.sub(r\"==+\\s*(.*?)\\s*==+\", r\"\\1\", text)\n",
        "\n",
        "    # 5. Убираем остатки тройных и двойных кавычек '' и '''\n",
        "    text = re.sub(r\"'{2,3}\", \"\", text)\n",
        "\n",
        "    # 6. Убираем конструкции типа \"| writer = \" или \"| composer = \" в начале текста\n",
        "    text = re.sub(r\"\\|\\s*\\w+\\s*=\\s*[^|}]*\", \"\", text)\n",
        "\n",
        "    # 7. Убираем двоеточия в начале строки (часто используются в текстах песен)\n",
        "    text = re.sub(r\"(?m)^:+\", \"\", text)\n",
        "\n",
        "    # 8. Преобразуем множественные пробелы и переносы строк в один пробел\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # 9. Убираем пробелы вначале и конце\n",
        "    text = text.strip()\n",
        "\n",
        "    return (titles, text)\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "# Применяем ко всем секциям\n",
        "wikipedia_sections = [clean_section_advanced(ws) for ws in wikipedia_sections]\n",
        "\n",
        "# Фильтруем короткие секции\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if len(ws[1]) >= 100]\n",
        "\n",
        "print(f\"Отфильтровано {original_num_sections-len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)} секций.\")\n"
      ],
      "metadata": {
        "id": "hkJO1o6coVb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "# Функция подсчета токенов\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция разделения строк\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter), пытаясь сбалансировать токены с каждой стороны.\"\"\"\n",
        "\n",
        "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # разделитель не найден\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # нет необходимости искать промежуточную точку\n",
        "    else:\n",
        "        # Считаем токены\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        # Предварительное разделение по середине числа токенов\n",
        "        best_diff = halfway\n",
        "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "# Функция обрезает строку до максимально разрешенного числа токенов\n",
        "def truncated_string(\n",
        "    string: str, # строка\n",
        "    model: str, # модель\n",
        "    max_tokens: int, # максимальное число разрешенных токенов\n",
        "    print_warning: bool = True, # флаг вывода предупреждения\n",
        ") -> str:\n",
        "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    # Обрезаем строку и декодируем обратно\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\")\n",
        "    # Усеченная строка\n",
        "    return truncated_string\n",
        "\n",
        "# Функция делит секции статьи на части по максимальному числу токенов\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], # секции\n",
        "    max_tokens: int = 1000, # максимальное число токенов\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    max_recursion: int = 5, # максимальное число рекурсий\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
        "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # Если длина соответствует допустимой, то вернет строку\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # иначе разделим пополам и выполним рекурсию\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]: # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
        "                continue\n",
        "            else:\n",
        "                # применим рекурсию на каждой половине\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1, # уменьшаем максимальное число рекурсий\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
      ],
      "metadata": {
        "id": "4oivfOi8oXj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Делим секции на части\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\")\n"
      ],
      "metadata": {
        "id": "vaqybqa3oZy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata  # ← Импортируем userdata из Colab\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Получаем API ключ из секретов Colab\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "RjihuYdtobOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"text\": wikipedia_strings})\n",
        "\n",
        "df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
        "\n",
        "SAVE_PATH = f\"./{name}_2025.csv\"\n",
        "# Сохранение результата\n",
        "df.to_csv(SAVE_PATH, index=False)"
      ],
      "metadata": {
        "id": "B-C-bAp3od1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "TCYJzuSNof7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}